---
layout: home
title: Stuart's PGCert Portfolio
---
* [Introduction](#introduction)
* [Reflective Area 1 – Operational Issues](#reflective-area-1--operational-issues)
* [Reflective Area 2 – Learning, Teaching and Assessment](#reflective-area-2--learning-teaching-and-assessment)
* [Reflective Area 3 – Wider Context](#reflective-area-3--wider-context)
* [Reflective Area 4 – Specialist Interest](#reflective-area-4--specialist-interest)
* [Professional Development Planning](#professional-development-planning)
* [Conclusion](#conclusion)
* [Bibliography](#bibliography)

## Introduction
<img
  class="align-left"
  src="{{ "/assets/media/Stuart_Smith_Digital_Skills_Developer.jpg" | prepend: site.baseurl }}" 
  alt="Stuart Smith Digital Skills Developer" 
  width="300" 
  loading="lazy" 
/>
My name is Stuart Smith, and I work as the Digital Skills Developer at the University of Greenwich—a unique role that focuses on how students engage with digital technologies both during their studies and in preparation for their future careers. This role builds on earlier work I’ve done in learning technology in both commercial and university settings.

Technology isn’t something I add into my work—it’s woven into it. My pedagogical stance is rooted in constructivist and humanist traditions (*Vygotsky, 1978; Freire, 1970; Rogers, 1969*), which emphasise learner agency, reflection, and the social context of knowledge-building. From the Microsoft 365 suite to tools like Visual Studio Code, Adobe Creative Cloud, and even my experiments with JavaScript and Python, I aim to model the kind of thoughtful digital practices I want students to develop. With the rise of generative AI, tools like Copilot have become central to my work. I see them as transformative—not just in terms of automation, but in terms of how we think, create, and support learners. I believe strongly in using the same tools I introduce to students, and I see my role as a guide helping them navigate this fast-evolving landscape.

## Reflective Area 1 – Operational Issues

A core part of my practice involves teaching students how to  integrate tools like Copilot and other digital technologies to support their everyday learning. I use Copilot regularly, especially in workshop scenarios. For example, when I was running an Excel tutorial and discovered that key learning materials had been deprecated, I used Copilot in real-time to generate a new learning activity based on prompts that students helped craft. This turned what could’ve been a failure into a moment of digital empowerment. Students saw that a lack of knowledge wasn't a dead end—it was a prompt for collaboration, experimentation, and co-creation.

Copilot excels in many areas—particularly as a personal tutor or coach. Its ability to understand natural language, propose ideas, and reframe problems makes it particularly useful for students with lower confidence or those from non-traditional learning backgrounds. But it’s not without constraints. Enterprise-level censorship, hallucinations, and limited philosophical depth can frustrate more advanced users. In particular, Microsoft has implemented filters in Copilot that block certain terms from being used, such as those associated with violence, politics, or elections, which can hinder academic inquiry in higher education (Field, 2024). In terms of ethical use, Copilot’s cheerful tone sometimes masks a lack of clarity about its boundaries, and it can become evasive when questioned about deeper topics.

Additionally I am implementing SCIM (System for Cross-domain Identity Management) and LTI (Learning Tools Interoperability) integrations to ensure improved access to LinkedIn Learning across Moodle. This technical foundation helped embed digital learning more meaningfully within the student experience—reducing login friction, improving visibility of learning resources, and enabling more flexible, student-led exploration of content within their academic modules. to ensure improved access to LinkedIn Learning across Moodle, enhancing user experience and reducing access barriers—an example of how thoughtful backend design can directly enhance learner engagement.

I led the development of the Digital Backpack in response to the withdrawal of the Jisc digital diagnostic tool, working cross-institutionally to address a critical gap in student skills assessment (*Jisc, 2019*). The project reflects my ability to manage complex TEL initiatives aligned with both pedagogical goals and institutional strategy. Through iterative consultation, I ensured the design was both pedagogically robust and operationally feasible.

Similarly, I use Visual Studio Code for more technical work—particularly for its support of Markdown and Jupyter notebooks. Markdown is one of the oldest and most robust text formats available and has particular value in education because of its accessibility, structure, and long-term archival viability. As Oelen and Auer (2019) note, "it is possible to create presentation slides without the need of visual interaction, making it suitable for non-sighted users," highlighting Markdown’s compatibility with screen readers and keyboard-only navigation. Unfortunately, Microsoft’s broader ecosystem isn’t Markdown-friendly. Word and Teams, for instance, don't offer clean Markdown workflows, which can present real limitations when working with screen readers or preparing structured content for future reuse.

[↑ Back to top](#introduction)

## Reflective Area 2 – Learning, Teaching and Assessment

All of my teaching currently takes place online, primarily through Microsoft Teams. I’ve adapted my sessions to be fully interactive, even when students don’t turn on their cameras. Tools like MentiMeter and Microsoft Forms allow me to create moments of live feedback and insight. These tools directly support a constructivist approach by enabling students to co-construct understanding through peer insight, shared questions, and formative, interactive contributions in real time. These tools help students who might otherwise be silent in a traditional classroom feel seen and heard. Menti’s anonymity, in particular, is powerful—it gives students permission to admit they don’t know something, or to experiment with an idea they aren’t confident in yet.

Copilot has become foundational to my teaching strategy. In an Excel session, for example, students were able to use Copilot to create custom tutorials based on their own goals and questions. This not only increased relevance but showed students how to take ownership of their learning journey. Separately, when demonstrating Microsoft Word’s Editor feature—which is distinct from Copilot and focuses on spelling, grammar, and style—many students were surprised to learn they already had access to such targeted support. These features are particularly empowering for those with lower academic English levels or dyslexia, providing real-time feedback that enhances writing quality without requiring generative input. This aligns with recent research showing that AI-enhanced tools, such as those that restructure and annotate text, can significantly improve the learning experience for students with dyslexia by reducing cognitive load and supporting comprehension (Zhao et al., 2025).

I’ve also noticed that TEL can be a double-edged sword. At a personal level the lack of student cameras can make teaching online feel isolating, and bandwidth limitations or noisy environments mean many students attend from less-than-ideal locations. However, by incorporating chat-based interaction, flexible feedback loops, and design that anticipates these limitations, we still create valuable experiences.

Inclusion remains a priority. Many of our students are multilingual, and some have experienced educational trauma. When one mature student, previously bullied in her workplace over digital skills, realised she could use Copilot to reframe complex information in her own words, she cried with relief. That moment reminded me of the profound emotional and psychological role TEL can play in helping people reclaim their agency.

[↑ Back to top](#introduction)

## Reflective Area 3 – Wider Context

Accessibility and data ethics are foundational concerns in my work. My MSc dissertation, *Can a Virtual Learning Environment Interface Meet the Needs of Dyslexics and Non-Dyslexics?* (Smith, 2001), focused on interface design for learners with specific needs, particularly around reducing a prescriptive approach to lesson design, often shaped by rigid or inflexible syllabus-driven models. Since then, I’ve contributed to JISC publications on accessibility (Smith, 2002). These principles continue to shape my TEL work today, especially in designing tools and resources that account for diverse learning styles and neurodiversity.&#x20;

I champion open formats like Markdown because they promote accessibility through structure and transparency (*Oelen and Auer, 2019*). Unlike proprietary file formats, Markdown works cleanly with assistive technologies and ensures content can be archived or shared without requiring expensive or restrictive platforms.

Ethically, we face difficult questions around AI. Scholars such as *Selwyn (2019)* and *Luckin (2018)* argue that while AI presents opportunities for personalised learning, it also raises issues around equity, bias, and educational values. Should students be allowed to write essays in their first language and use AI to translate into English? Current policy says no—but we admit students knowing they don’t yet have academic English proficiency. If we're not willing to allow AI-assisted translation, are we also willing to ban private tutoring, which creates far greater inequality? These are the kinds of dilemmas we need to address honestly and empathetically.

On data privacy: institutional systems often don’t talk to each other. Staff end up replicating student data in insecure ways—OneDrive folders, spreadsheets, emails—creating inconsistent practices and risk. Students also engage in similar behaviours, often without understanding the implications or legal responsibilities. I teach students that managing information isn’t about hoarding—it’s about stewardship: knowing where your data is, why it’s there, and who can access it. This sits alongside my wider work in promoting open standards and accessible formats. Both data practices and content choices reflect a deeper commitment to transparency, control, and learner autonomy.

Many students at Greenwich face significant digital inequality. My work on the Digital Backpack was designed to acknowledge and respond to this reality, offering students a scaffolded, non-judgemental path toward digital competence using familiar platforms like Microsoft 365 and LinkedIn Learning.

In designing the Digital Backpack, I ensured that the student data gathered is not used summatively or punitively. This decision was critical not only for GDPR compliance, but also for establishing trust and promoting genuine engagement—a principle that should underpin all TEL design.

This is an undeniably exciting time for artificial intelligence in education. The field is evolving rapidly, offering unprecedented opportunities for personalisation, automation, and insight. However, it differs significantly from earlier digital transformations—such as the rise of the web—which were largely built on open-source principles and decentralised infrastructure. Generative AI, by contrast, demands vast computing power and costly infrastructure. While open-source models are emerging and can be adapted or fine-tuned locally, the dominant players—such as Copilot, ChatGPT, and Gemini—remain proprietary, closed-source systems that are expensive to access and tightly controlled by large corporations. This raises important questions for education. If institutions embed AI tools deeply into their teaching and operational practices, what happens when those tools are owned and governed by profit-driven companies? As Bill Gates observed, the AI provider that builds the most effective personal agent may come to dominate how users access and interact with digital services. For education, this prompts careful consideration: should we rely so heavily on one supplier? Can we maintain flexibility, transparency, and pedagogical integrity in an AI landscape that is increasingly centralised?

> “There will be one company that creates a personal agent that will understand all your activities... That’s a big thing because you’ll never go to a search site again. You’ll never go to a productivity tool again. You’ll never go to Amazon again. Everything will be mediated through your agent... Whoever wins that will take the current dispersed, high profit areas, and move them into a single area.”
> — Gates, B. (2023)

[↑ Back to top](#introduction)

## Reflective Area 4 – Specialist Interest

My core area of specialist interest is the intersection of generative AI, immersive environments, and personalised learning. I’m especially drawn to open-source AI models that can run locally and be combined with Retrieval-Augmented Generation (RAG) frameworks. With these tools, we can let students interrogate large sets of course data, reading lists, and materials using natural language—in their own voice and at their own pace.

I see huge potential for building learning companions that aren’t just reactive but contextual. Imagine a student in an immersive VR space, moving through course content visually, spatially, and interactively—asking questions, testing ideas, and receiving tailored responses from a localised, privacy-respecting AI tutor. It’s an ambitious vision, but one that feels like the natural evolution of TEL.

I'm also concerned about the lack of open standards. *Knox (2020)* highlights how AI integration into education systems can reflect national and corporate priorities, potentially compromising educational autonomy. While tools like Copilot are powerful, their opacity and corporate ties limit their educational potential. We need transparent, accountable AI systems if we want to support academic freedom and innovation. I believe in technology as a democratising force—but only if it remains open, ethical, and student-focused.

[↑ Back to top](#introduction)

## Professional Development Planning

My next formal step is applying for CMALT, which I see as a way to consolidate and articulate the TEL journey I’m already on. I’m fortunate to work alongside several Certified Members of ALT, and I look forward to joining that professional community.

Longer term, I’d like to explore collaborative research around generative AI in education, including projects like the Digital Backpack—our digital capability assessment and credentialing tool. I also want to improve my programming fluency so I can prototype some of the learning tools I imagine. Right now, Generative AI itself is helping me do that, guiding me through Python and JavaScript as I experiment.

Eventually, I’d like to contribute more actively to the field through publications and collaborative tool-building. I’m even considering a PhD, though I’m more focused in the short term on meaningful, hands-on development that empowers students and reimagines what assessment and learning could be.

[↑ Back to top](#introduction)

## Conclusion

Before beginning this PGCert, I didn’t value reflection as much as I do now. Having a scaffolded portfolio has helped me see its importance—not just for academic growth, but for refining my own thinking. I plan to continue this habit well beyond the PGCert, using tools like Jekyll and Markdown to build a sustainable, portable professional portfolio.

This process has reinforced many of my core beliefs: that digital tools can democratise learning, that students need agency, and that institutions must do better in protecting and empowering learners. It’s also deepened my interest in systemic innovation—from immersive learning to ethical AI, from accessibility to data minimalism.

I’ve come away from this with a stronger sense of direction and a renewed belief that, while technology often introduces new challenges, it also opens doors for creativity, access, and transformation—especially when guided by thoughtful educators.

[↑ Back to top](#introduction)

## Bibliography

Oelen, A. and Auer, S. (2019) ‘Content Authoring with Markdown for Visually Impaired and Blind Users’, *Proceedings of the 16th International Web for All Conference*, ACM. Available at: [https://doi.org/10.1145/3315002.3317576](https://doi.org/10.1145/3315002.3317576) (Accessed: 7 May 2025).

Smith, S. (2002) ‘Dyslexia and virtual learning environment interfaces’, *Access all areas: disability, technology and learning*, pp. 50–53.

Zhao, W., Li, R., Hou, Y., Sun, Q., Zhang, H. and Ren, X. (2025) ‘Let AI Read First: Enhancing Reading Abilities for Individuals with Dyslexia through Artificial Intelligence’, *arXiv preprint*. Available at: [https://arxiv.org/abs/2504.00941](https://arxiv.org/abs/2504.00941) (Accessed: 7 May 2025).

Field, H. (2024) ‘Microsoft is blocking terms that cause its AI to create violent images’, *CNBC*, 8 March. Available at: [https://www.cnbc.com/2024/03/08/microsoft-blocking-terms-that-cause-its-ai-to-create-violent-images.html](https://www.cnbc.com/2024/03/08/microsoft-blocking-terms-that-cause-its-ai-to-create-violent-images.html) (Accessed: 7 May 2025).

Smith, S.M. (2001) *Can a Virtual Learning Environment Interface Meet the Needs of Dyslexics and Non-Dyslexics?* MSc Computing Science Dissertation. Staffordshire University.

Freire, P. (1970) *Pedagogy of the Oppressed*. New York: Herder and Herder.

Gates, B. (2023) *Bill Gates on the AI revolution*. Goldman Sachs. Available at: [https://www.goldmansachs.com/insights/articles/bill-gates-on-the-ai-revolution](https://www.goldmansachs.com/insights/articles/bill-gates-on-the-ai-revolution) (Accessed: 6 May 2025).

Jisc (2019) *Building Digital Capability: The six elements defined*. Available at: [https://www.jisc.ac.uk/rd/projects/building-digital-capability](https://www.jisc.ac.uk/rd/projects/building-digital-capability) (Accessed: 7 May 2025).

Knox, J. (2020) ‘Artificial intelligence and education in China’, *Learning, Media and Technology*, 45(3), pp. 295–308. doi:10.1080/17439884.2020.1740652.

Luckin, R. (2018) *Machine Learning and Human Intelligence: The Future of Education for the 21st Century*. London: UCL IOE Press.

Rogers, C.R. (1969) *Freedom to Learn*. Columbus, OH: Merrill.

Selwyn, N. (2019) *Should Robots Replace Teachers? AI and the Future of Education*. Cambridge: Polity Press.

Vygotsky, L.S. (1978) *Mind in Society: The Development of Higher Psychological Processes*. Cambridge, MA: Harvard University Press.
